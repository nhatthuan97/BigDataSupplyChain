import warnings
warnings.filterwarnings('ignore')



import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, r2_score
import numpy as np
from math import sqrt

def analyze_prepared_data(df, title):
    # Specify features and target based on the prepared DataFrame columns
    features = [col for col in df.columns if col != 'Actual Lead Time']
    target = 'Actual Lead Time'
    
    # Initialize lists to store metrics
    min_errors = []
    max_errors = []
    average_errors = []
    median_errors = []
    rmses = []
    r2_scores = []

    # Number of runs
    n_runs = 10

    for _ in range(n_runs):
        # Split the data into training and testing sets
        X_train, X_test, y_train, y_test = train_test_split(df[features], df[target], test_size=0.2, random_state=None)
        
        # Define and train the model
        model = RandomForestRegressor(random_state=None)  
        model.fit(X_train, y_train.values.ravel())
        
        # Make predictions
        y_pred = model.predict(X_test)
        
        # Calculate errors
        errors = np.abs(y_test.values.ravel() - y_pred)
        
        # Collect metrics
        min_errors.append(np.min(errors))
        max_errors.append(np.max(errors))
        average_errors.append(np.mean(errors))
        median_errors.append(np.median(errors))
        rmses.append(sqrt(mean_squared_error(y_test, y_pred)))
        r2_scores.append(r2_score(y_test, y_pred))

    '''
    # Prepare data for plotting
    metrics = [ min_errors,max_errors,average_errors, median_errors, rmses, r2_scores]
    metric_names = ['Min Error','Max Error', 'Average Error', 'Median Error', 'RMSE', 'R-squared']

    # Plotting
    fig, axs = plt.subplots(nrows=1, ncols=len(metric_names), figsize=(20, 6), sharey=False)
    for i, metric in enumerate(metrics):
        axs[i].boxplot(metric)
        axs[i].set_title(metric_names[i])

    plt.tight_layout()
    plt.suptitle(title+': Performance Metrics Across 10 Runs', fontsize=16, y=1.02)  # Add suptitle
    plt.show()
    '''
    metrics = {
        'Min Error': min_errors,
        'Max Error': max_errors,
        'Average Error': average_errors,
        'Median Error': median_errors,
        'RMSE': rmses,
        'R-squared': r2_scores
    }

    # Calculate and display summary statistics
    summary_stats = {}
    for metric_name, values in metrics.items():
        summary_stats[metric_name] = {
            'Mean': np.mean(values),
            'Std Dev': np.std(values),
            'Min': np.min(values),
            '50%': np.median(values),
            'Max': np.max(values)
        }

    summary_df = pd.DataFrame(summary_stats).T
    print(summary_df)




    # Identify and plot effects of the top 5 most important features
    top_indices = np.argsort(model.feature_importances_)[-5:]
    top_features = [features[i] for i in top_indices]

    top_importances = model.feature_importances_[top_indices]

    # Plotting
    plt.figure(figsize=(10, 6))
    plt.barh(top_features, top_importances, color='skyblue')
    plt.xlabel('Feature Importance', fontsize=18)  # Increase font size of x-axis label
    plt.ylabel('Features', fontsize=18)            # Increase font size of y-axis label
    plt.title(title, fontsize=18)                  # Increase font size of title
    plt.xticks(fontsize=18)                        # Increase font size of x-axis tick labels
    plt.yticks(fontsize=18)                        # Increase font size of y-axis tick labels
    plt.show()

    


import pandas as pd

def df_date_filter(df, start, end):
    """
    Filters the provided DataFrame for entries between the specified start and end dates.

    Parameters:
        df (DataFrame): The DataFrame to filter.
        start (str or pd.Timestamp): The start date for the filter.
        end (str or pd.Timestamp): The end date for the filter.

    Returns:
        DataFrame: A filtered DataFrame containing only the entries within the specified date range.
    """

    # Create the mask for the date range
    mask = (df['Order Entry Date'] >= start) & (df['Order Entry Date'] <= end)
    filtered_df = df.loc[mask]
    categories = ['Country', 'Transportation Mode', 'Order Type', 'Fulfillment Method', 'Product Category', 
            'Vendor Incoterm', 'Reason Code', 'Item Tracer Category','Framework Contract']
    numeric=['Manufacture','Pick Up','Quality Assurance','Illustrative Price',
         'RO Validation','Sourcing and Planning','USAID Approval','Process PO/DO','Reason Code Duration']
    features=categories+numeric
    target = ['Actual Lead Time']
    
    df_train = filtered_df[features+target]
    df_train.dropna(inplace=True)

    '''
    # Calculate the IQR bounds for each numeric column
    bounds = {}
    for column in numeric:
        Q1 = df_train[column].quantile(0.25)
        Q3 = df_train[column].quantile(0.75)
        IQR = Q3 - Q1
        lower_bound = Q1 - 1.5 * IQR
        upper_bound = Q3 + 1.5 * IQR
        bounds[column] = (lower_bound, upper_bound)
    
    # Apply the IQR filters to the DataFrame
    for column, (lower_bound, upper_bound) in bounds.items():
        df_train = df_train[(df_train[column] >= lower_bound) & (df_train[column] <= upper_bound)]
    '''
    '''
    Q1 = df_train['Actual Lead Time'].quantile(0.25)
    Q3 = df_train['Actual Lead Time'].quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    df_train = df_train[(df_train['Actual Lead Time'] >= lower_bound) & (df_train['Actual Lead Time'] <= upper_bound)]
    '''
    return df_train





import pandas as pd
#Read from file
df=pd.read_csv('USAID_Health_Comodity_Delivery.csv')

numeric_cols = df.select_dtypes(include=['number']).columns

# Replace negative values in numeric columns with NaN
df[numeric_cols] = df[numeric_cols].applymap(lambda x: x if x >= 0 else None)


df['Latest Actual Delivery Date'] = pd.to_datetime(df['Latest Actual Delivery Date'])
df['Order Entry Date'] = pd.to_datetime(df['Order Entry Date'])

# Create the new column
df['Actual Lead Time'] = df['Latest Actual Delivery Date'] - df['Order Entry Date']
df['Actual Lead Time'] = df['Actual Lead Time'].dt.days

#Modify the reason code
df['Reason Code'] = df['Reason Code'].str.slice(0, 2)

categories = ['Country', 'Transportation Mode', 'Order Type', 'Fulfillment Method', 'Product Category', 
        'Vendor Incoterm', 'Reason Code', 'Item Tracer Category','Framework Contract']
numeric=['Manufacture','Pick Up','Quality Assurance','Illustrative Price',
        'RO Validation','Sourcing and Planning','USAID Approval','Process PO/DO','Reason Code Duration']




# Label econding for the categorical data in df
from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()
for i in categories:
    df[i] = le.fit_transform(df[i])

#For every nan value use the average of the column group by 'Product Category'
for i in numeric:
    df[i] = df[i].fillna(df.groupby('Product Category')[i].transform('mean'))




#Some analysis for reliability
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from statsmodels.stats.outliers_influence import variance_inflation_factor
from sklearn.preprocessing import StandardScaler
import researchpy as rp

# Load dataset
data = df_date_filter(df, df['Order Entry Date'].min(), df['Order Entry Date'].max())
data=data[numeric]
print(len(data))
# Summary Statistics
summary_stats = data.describe()

# Correlation Matrix
correlation_matrix = data.corr()
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm')
plt.show()

# Variance Inflation Factor (VIF)
def calculate_vif(df):
    vif = pd.DataFrame()
    vif["Variable"] = df.columns
    vif["VIF"] = [variance_inflation_factor(df.values, i) for i in range(df.shape[1])]
    return vif

# Assuming 'data' is preprocessed and only includes numerical columns
vif = calculate_vif(data.select_dtypes(include=[np.number]))


# Output results
print("Summary Statistics:\n", summary_stats)
print("VIF:\n", vif)



# Filter dataframes
whole_df = df_date_filter(df, df['Order Entry Date'].min(), df['Order Entry Date'].max())
after_covid_df = df_date_filter(df, '2020-01-01', df['Order Entry Date'].max())
before_covid_df = df_date_filter(df, df['Order Entry Date'].min(), '2020-01-01')
peak_df = df_date_filter(df, '2020-01-01', '2022-03-22')

# Using df_date_filter to get before and after peak periods separately
before_peak_df = df_date_filter(df, df['Order Entry Date'].min(), '2019-12-31')
after_peak_df = df_date_filter(df, '2022-03-23', df['Order Entry Date'].max())

# Concatenating both DataFrames to create a non-peak DataFrame
non_peak_df = pd.concat([before_peak_df, after_peak_df], ignore_index=True)

# Optionally, you can print the number of records in each DataFrame to validate the splits
print("Whole period records:", len(whole_df))
print("After COVID records:", len(after_covid_df))
print("Before COVID records:", len(before_covid_df))
print("Peak period records:", len(peak_df))
print("Non-peak period records:", len(non_peak_df))



analyze_prepared_data(whole_df,'Default')
analyze_prepared_data(after_covid_df,'AfterCovid')
analyze_prepared_data(before_covid_df,'Before Covid')
analyze_prepared_data(peak_df,'Peak Pandemic')
analyze_prepared_data(non_peak_df,'Non Peak Pandemic')
